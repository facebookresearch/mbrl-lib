{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview\n",
    "\n",
    "In this example, we are going to use our toolbox to write the [PETS](https://arxiv.org/pdf/1805.12114.pdf) algorithm (Chua at al., 2018), and use it to solve a continuous version of the cartpole environment. PETS is a model-based algorithm that consists of two main components: an ensemble of probabilistic models (each a feed-forward neural network), and a planner using the [Cross-Entropy Method](https://people.smp.uq.edu.au/DirkKroese/ps/aortut.pdf) (de Boer et al., 2004). \n",
    "\n",
    "A basic implementation of this algorithm consists of the following sequence of steps:\n",
    "\n",
    "1. Gather data using an exploration policy\n",
    "2. Repeat:<br>\n",
    "  2.1. Train the dynamics model using all available data.<br>\n",
    "  2.2. Do a trajectory on the environment, choosing actions with the planner, using the dynamics model to simulate environment transitions.\n",
    "  \n",
    "The ensemble model is trained to predict the environment's dynamics, and the planner tries to find high-reward trajectories over the model dynamics. \n",
    "\n",
    "To implement this using `MBRL-Lib`, we will use an ensemble of Gaussian models (available in the [mbrl.models](https://luisenp.github.io/mbrl-lib/models.html#mbrl.models.GaussianMLP) module), and a trajectory optimizer agent that uses CEM (available in the [mbrl.planning](https://luisenp.github.io/mbrl-lib/planning.html#mbrl.planning.TrajectoryOptimizerAgent) module). We will also rely on several of the utilities avaiable in the [mbrl.util](https://luisenp.github.io/mbrl-lib/util.html) module. Finally, we will use wrap the dynamics model into a [gym-like environment](https://luisenp.github.io/mbrl-lib/models.html#mbrl.models.ModelEnv) over which we can plan action sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import omegaconf\n",
    "\n",
    "import mbrl.env.cartpole_continuous as cartpole_env\n",
    "import mbrl.env.reward_fns as reward_fns\n",
    "import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util as util\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the environment\n",
    "\n",
    "First we instantiate the environment and specify which reward function and termination function to use with the gym-like model wrapper, along with some utility objects. The termination function tells the wrapper if an observation should cause an episode to end or not, and it is an input used in some algorithms, like [MBPO](https://github.com/JannerM/mbpo/blob/master/mbpo/static/halfcheetah.py). The reward function is used to compute the value of the reward given an observation, and it's used by some algorithms, like [PETS](https://github.com/kchua/handful-of-trials/blob/77fd8802cc30b7683f0227c90527b5414c0df34c/dmbrl/controllers/MPC.py#L65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/home/lep/.conda/envs/mbrl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "env = cartpole_env.CartPoleEnv()\n",
    "env.seed(seed)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "\n",
    "# This functions allows the model to evaluate the true rewards given an observation \n",
    "reward_fn = reward_fns.cartpole\n",
    "# This function allows the model to know if an observation should make the episode end\n",
    "term_fn = termination_fns.cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydra configuration\n",
    "\n",
    "MBRL-Lib uses [Hydra](https://github.com/facebookresearch/hydra) to manage configurations. For the purpose of this example, you can think of the configuration object as a dictionary with key/value pairs--and equivalent attributes--that specify the model and algorithmic options. Our toolbox expects the configuration object to be organized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_length = 200\n",
    "num_trials = 10\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# Everything with \"???\" indicates an option with a missing value.\n",
    "# Our utility functions will fill in these details using the \n",
    "# environment information\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": {\n",
    "        \"model\": {\n",
    "            \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "            \"device\": device,\n",
    "            \"num_layers\": 3,\n",
    "            \"ensemble_size\": 5,\n",
    "            \"hid_size\": 200,\n",
    "            \"use_silu\": True,\n",
    "            \"in_size\": \"???\",\n",
    "            \"out_size\": \"???\",\n",
    "            \"deterministic\": False,\n",
    "            \"propagation_method\": \"fixed_model\"\n",
    "        }\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_trials\": num_trials,\n",
    "        \"model_batch_size\": 256,\n",
    "        \"validation_ratio\": 0.05\n",
    "    }\n",
    "}\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Note: </b> This example uses a probabilistic ensemble. You can also use a fully deterministic model with class GaussianMLP by setting ensemble_size=1, and deterministic=False. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a dynamics model\n",
    "\n",
    "Given the configuration above, the following two lines of code create a wrapper for proprioceptive models, and a gym-like environment that wraps it, which we can use for simulating the real environment. The proprioceptive model wrapper takes care of creating input/output data tensors to the underlying NN model (by concatenating observations, actions and rewards appropriately), normalizing the input data to the model, and other data processing tasks (e.g., converting observation targets to deltas with respect to the input observation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a proprioceptive model wrapper for this environment\n",
    "dynamics_model = util.create_proprioceptive_model(cfg, obs_shape, act_shape)\n",
    "\n",
    "# Create a gym-like environment to encapsulate the model\n",
    "model_env = models.ModelEnv(env, dynamics_model, term_fn, reward_fn, rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset object\n",
    "\n",
    "Creating training and validation datasets can be done with a single line of code. The capacity of the training dataset will be ``(1 - V) * trial_length * num_trials``, and that of the validation set will be ``V * trial_length * num_trials``, where ``V`` is the validation ratio specified in the ``cfg.overrides`` group. Since we want to train an ensemble model, as specified in the ``dynamics_model.model`` option, we pass ``train_is_bootstrap=True`` to create a replay buffer that can be used to train this type of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val = util.create_replay_buffers(\n",
    "    cfg, obs_shape, act_shape, train_is_bootstrap=True, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now populate the replay buffer with random trajectories of a desired length, using a single function call to `util.rollout_agent_trajectories`. Note that we pass an agent of type `planning.RandomAgent` to generate the actions; however, this method accepts any agent that is a subclass of `planning.Agent`, allowing changing exploration strategies with minimal changes to the code. \n",
    "\n",
    "This method will also randomly populate the validation replay buffer, instead of the training buffer, according to `cfg.overrides.validation_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training samples 190\n",
      "# validation samples 10\n"
     ]
    }
   ],
   "source": [
    "util.rollout_agent_trajectories(\n",
    "    env,\n",
    "    trial_length, # initial exploration steps\n",
    "    planning.RandomAgent(env),\n",
    "    {},\n",
    "    rng,\n",
    "    train_dataset=dataset_train,\n",
    "    val_dataset=dataset_val,\n",
    "    val_ratio=cfg.overrides.validation_ratio,\n",
    "    trial_length=trial_length\n",
    ")\n",
    "\n",
    "print(\"# training samples\", dataset_train.num_stored)\n",
    "print(\"# validation samples\", dataset_val.num_stored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEM Agent\n",
    "\n",
    "The following config object and the subsequent function call create an agent that can plan using the Cross-Entropy Method over the model environment created above. When calling `planning.create_trajectory_optim_agent_for_model`, we also specify how many particles to use when propagating model uncertainty, as well as the uncertainty propagation method, \"fixed_model\", which corresponds to TSinf in the PETS paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_cfg = omegaconf.OmegaConf.create({\n",
    "    # this class evaluates many trajectories and picks the best one\n",
    "    \"_target_\": \"mbrl.planning.TrajectoryOptimizerAgent\",\n",
    "    \"planning_horizon\": 15,\n",
    "    \"replan_freq\": 1,\n",
    "    \"verbose\": False,\n",
    "    \"action_lb\": \"???\",\n",
    "    \"action_ub\": \"???\",\n",
    "    # this is the optimizer to generate and choose a trajectory\n",
    "    \"optimizer_cfg\": {\n",
    "        \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "        \"num_iterations\": 5,\n",
    "        \"elite_ratio\": 0.1,\n",
    "        \"population_size\": 500,\n",
    "        \"alpha\": 0.1,\n",
    "        \"device\": device,\n",
    "        \"lower_bound\": \"???\",\n",
    "        \"upper_bound\": \"???\"\n",
    "    }\n",
    "})\n",
    "\n",
    "agent = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running PETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a model and an agent, we can know run PETS with a simple loop and a few function calls. The first code block creates a callback to pass to the model trainer to accumulate the training losses and validation scores observed. The second block is just an utility function to update the agent's visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_scores = []\n",
    "\n",
    "def train_callback(_model, _total_calls, _epoch, tr_loss, _tr_score, val_score, _best_val):\n",
    "    train_losses.append(tr_loss)\n",
    "    val_scores.append(val_score.mean())   # this returns val score per ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_axes(_axs, _frame, _text, _trial, _steps_trial, _all_rewards, force_update=False):\n",
    "    if not force_update and (_steps_trial % 10 != 0):\n",
    "        return\n",
    "    _axs[0].imshow(_frame)\n",
    "    _axs[0].set_xticks([])\n",
    "    _axs[0].set_yticks([])\n",
    "    _axs[1].clear()\n",
    "    _axs[1].set_xlim([0, num_trials + .1])\n",
    "    _axs[1].set_ylim([0, 200])\n",
    "    _axs[1].set_xlabel(\"Trial\")\n",
    "    _axs[1].set_ylabel(\"Trial reward\")\n",
    "    _axs[1].plot(_all_rewards, 'bs-')\n",
    "    _text.set_text(f\"Trial {_trial + 1}: {_steps_trial} steps\")\n",
    "    display.display(plt.gcf())  \n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines implement the PETS algorith. First, we create a model trainer and pass some hyperparameters for the optimizer (Adam), along with references to the model instance and datasets to use. Then we start a loop where we execute actions of ``agent`` in the environment and train the model at the beginning of the episode (by calling ``model_trainer.train()``. At every step in the loop, we execute an agent action in the environment and populate the datasets by calling ``util.step_env_and_populate_dataset()``. Importantly, at the beginning of each episode we also call ``agent.reset()`` to clear any episode dependent cache; in the case of a ``TrajectoryOptimizerAgent``, this means clearing the previous action sequence found, which is shifted at every call to obtain an initial solution for the optimizer. \n",
    "\n",
    "The rest of the code is mostly bookkeeping to keep track of the total reward observed during each episode, and to make sure episodes terminate after some desired length. After running this code, you should see the agent reaching the maximum reward of 200 after a few episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x270 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a trainer for the model\n",
    "model_trainer = models.DynamicsModelTrainer(\n",
    "    dynamics_model, dataset_train, dataset_val=dataset_val, optim_lr=7.5e-4, weight_decay=3e-5)\n",
    "\n",
    "# Create visualization objects\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "ax_text = axs[0].text(300, 50, \"\")\n",
    "    \n",
    "# Main PETS loop\n",
    "all_rewards = [0]\n",
    "for trial in range(num_trials):\n",
    "    obs = env.reset()    \n",
    "    agent.reset()\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "    update_axes(axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, all_rewards)\n",
    "    while not done:\n",
    "        # --------------- Model Training -----------------\n",
    "        if steps_trial == 0:\n",
    "            dynamics_model.update_normalizer(dataset_train.get_all())  # update normalizer stats\n",
    "            model_trainer.train(num_epochs=50, patience=50, callback=train_callback)\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        next_obs, reward, done, _ = util.step_env_and_populate_dataset(\n",
    "            env,\n",
    "            obs,\n",
    "            agent,\n",
    "            {},\n",
    "            dataset_train,\n",
    "            dataset_val,\n",
    "            True, # whether to increase the validation set or not\n",
    "            cfg.overrides.validation_ratio,\n",
    "            rng\n",
    "        )\n",
    "            \n",
    "        update_axes(axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, all_rewards)\n",
    "        \n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "        \n",
    "        if steps_trial == trial_length:\n",
    "            break\n",
    "    \n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "update_axes(axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, all_rewards, force_update=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, below we check the results of the trainer callback, which show the training loss and validation score accross all calls to ``model_trainer.train()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10))\n",
    "ax[0].plot(train_losses)\n",
    "ax[0].set_xlabel(\"Total training epochs\")\n",
    "ax[0].set_ylabel(\"Training loss (avg. NLL)\")\n",
    "ax[1].plot(val_scores)\n",
    "ax[1].set_xlabel(\"Total training epochs\")\n",
    "ax[1].set_ylabel(\"Validation score (avg. MSE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to learn more about MBRL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about the other features of the library, please check out our [documentation](https://luisenp.github.io/mbrl-lib/). Also take a look at our provided implementations of [PETS](https://github.com/luisenp/mbrl-lib/blob/master/mbrl/algorithms/pets.py) and [MBPO](https://github.com/luisenp/mbrl-lib/blob/master/mbrl/algorithms/mbpo.py), and their configuration [files](https://github.com/luisenp/mbrl-lib/tree/master/conf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbrl",
   "language": "python",
   "name": "mbrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
